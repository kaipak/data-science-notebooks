{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f386463-67b6-4c7f-bfd6-eabb472fb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbfb2c-8a78-4cb6-bcbe-0385e63e4ab9",
   "metadata": {},
   "source": [
    "## Conversion to SimCLRv2 and Converting TF Pretrained Weights\n",
    "Pretrained weights can be found on Google's [repo](https://github.com/google-research/simclr). With conversion scripts linked. Most of the inital work can be found in spijkervet_prototypes.ipynb. This work is to clean up the spaghetti code and turn into modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a26a8-2db6-4c3a-801e-9cea8557459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.insert(0, '../../../SimCLRv2-PyTorch/')\n",
    "\n",
    "from utils.model import save_model, load_optimizer\n",
    "from simclr.modules import LogisticRegression\n",
    "from simclr import SimCLRv2, SimCLRv2_ft\n",
    "from simclr.modules import get_resnet, NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from utils import yaml_config_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5924409-4624-47a5-8947-3222c62f8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_repo = Path('/home/kaipak/dev/SimCLRv2-PyTorch/')\n",
    "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
    "config = yaml_config_hook(simclr_repo / 'config/config.yaml')\n",
    "tb_out = Path('/home/kaipak/models/tensorboard_logs')\n",
    "\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "    \n",
    "args = parser.parse_args([])\n",
    "args.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbf396-7d6c-40d7-8a95-eb5cda64c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.logistic_batch_size = 96\n",
    "args.resnet = \"resnet50\"\n",
    "args.epochs = 400\n",
    "args.gpus = 4\n",
    "args.optimizer = 'LARS'\n",
    "args.workers = 64\n",
    "args.dataset = 'CIFAR100'\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a2052-61f9-4257-bca4-982a64f09706",
   "metadata": {},
   "source": [
    "## Dataset Transforms\n",
    "Dataset loader calls below apply SimCLR paper recommended transforms producing $x_i$ $x_j$ pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876c5c5-fd92-41e6-bbec-3a57471d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.dataset == \"STL10\":\n",
    "    train_dataset = torchvision.datasets.STL10(\n",
    "        args.dataset_dir,\n",
    "        split=\"unlabeled\",\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size),\n",
    "    )\n",
    "elif args.dataset == \"CIFAR10\":\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        args.dataset_dir,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size),\n",
    "    )\n",
    "elif args.dataset == \"CIFAR100\":\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        args.dataset_dir,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size),\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if args.nodes > 1:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=True\n",
    "    )\n",
    "else:\n",
    "    train_sampler = None\n",
    "\n",
    "\n",
    "# Data Transforms happen here.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    drop_last=True,\n",
    "    num_workers=args.workers,\n",
    "    sampler=train_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d91b6-606a-4f7b-a824-0102cd7ca54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3f7ffc-3f27-46d9-b2a7-a76824c5942e",
   "metadata": {},
   "source": [
    "## SimCLRv2: Self Supervised Learning\n",
    "Modified SimCLR Pytorch code to v2 with Resnet code from converter which includes contrastive head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01870e1-d81a-461a-b2c7-1b7aa0521a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimCLRv2(resnet_depth=50, resnet_width_multiplier=2)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "if args.reload:\n",
    "    model_fp = os.path.join(\n",
    "        args.model_path, f\"checkpoint_{args.epoch_num}.tar\"\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_fp, map_location=args.device.type))\n",
    "\n",
    "model = model.to(args.device)\n",
    "optimizer, scheduler = load_optimizer(args, model)\n",
    "criterion = NT_Xent(args.batch_size, args.temperature, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98c3cf-5a65-429a-bd5c-abc345aaa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_loader, model, criterion, optimizer, writer, display_every=50):\n",
    "    \"\"\"Train function\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.cuda(non_blocking=True)\n",
    "        x_j = x_j.cuda(non_blocking=True)\n",
    "        \n",
    "        # Positive pair with encoding\n",
    "        h_i, h_j, z_i, z_j = model(x_i, x_j)\n",
    "        \n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % display_every == 0:\n",
    "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
    "        epoch_loss += loss.item()\n",
    "        args.global_step += 1\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573bdad-e6f8-48ef-a72a-7cc0afbccc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.global_step = 0\n",
    "args.current_epoch = 0\n",
    "tb_writer =  SummaryWriter(log_dir=f'/home/kaipak/models/tensorboard_logs/' +\n",
    "                           f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    epoch_loss = train(args, train_loader, model, criterion, optimizer, tb_writer)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {epoch_loss / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
    "    )\n",
    "    args.current_epoch += 1\n",
    "\n",
    "save_model(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8414383-2ba3-4bbb-a0ce-6987f0926130",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29f0dc-ec7f-4096-b4e7-77b35de6e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('/home/kaipak/models/SimCLRv2/r50_2x_sk1.pth').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702ff22-bfb0-4e61-82b1-06cc59620405",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25df3e-329c-4193-9749-3ec0e8359f6c",
   "metadata": {},
   "source": [
    "## SimCLRv2: Fine Tuning From Projection Head\n",
    "v2 says we should fine tune from middle projection layer. Original SimCLR implementation basically throws this away and additionally does not have fine-tuning step from Resnet. Build code to take middle layer of projection then run supervised fine-tuning using cross-entropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5259292-1452-48b9-9b9a-74965934e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tuning, we just need the standard images with resizing\n",
    "\n",
    "if args.dataset == \"STL10\":\n",
    "    train_dataset = torchvision.datasets.STL10(\n",
    "        args.dataset_dir,\n",
    "        split=\"train\",\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.STL10(\n",
    "        args.dataset_dir,\n",
    "        split=\"test\",\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "elif args.dataset == \"CIFAR10\":\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        args.dataset_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        args.dataset_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "elif args.dataset == \"CIFAR100\":\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        args.dataset_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        args.dataset_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.logistic_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=args.workers,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    #batch_size=args.logistic_batch_size,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfbfb2-f5e8-45a2-ba66-aec8fe6e2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 100\n",
    "simclr_model = SimCLRv2(resnet_depth=50, resnet_width_multiplier=2, sk_ratio=0.0625, \n",
    "                        pretrained_weights='/home/kaipak/models/SimCLRv2/r50_2x_sk1.pth')\n",
    "simclr_model_ft = SimCLRv2_ft(simclr_model, n_classes)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  simclr_model_ngpu = nn.DataParallel(simclr_model_ft)\n",
    "\n",
    "simclr_model = simclr_model_ngpu.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970a9d7-971b-45a3-a7fc-ea5b6bceba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, loader, model, criterion, optimizer, writer):\n",
    "    \"\"\"Train evaluation model\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, input in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = input\n",
    "        X = X.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "        \n",
    "        output = model(X)\n",
    "        step_loss = criterion(output, y)\n",
    "        \n",
    "        predicted = output.argmax(1)\n",
    "        step_accuracy = (predicted == y).sum().item() / y.size(0)\n",
    "        epoch_accuracy += step_accuracy\n",
    "        \n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += step_loss\n",
    "        writer.add_scalar(\"Accuracy/train_step\", step_accuracy, args.global_step)\n",
    "        args.global_step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Accuracy {step_accuracy}...\")\n",
    "        \n",
    "    writer.add_scalar(\"Accuracy/train_epoch\", step_accuracy, args.current_epoch)\n",
    "    writer.add_scalar(\"Loss/train_epoch\", epoch_loss, args.current_epoch)\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def test(args, loader, model, criterion, optimizer):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        x = x.to(args.device)\n",
    "        y = y.to(args.device)\n",
    "        \n",
    "        output = model(x)\n",
    "        step_loss = criterion(output, y)\n",
    "        \n",
    "        predicted = output.argmax(1)\n",
    "        step_accuracy = (predicted == y).sum().item() / y.size(0)\n",
    "        epoch_accuracy += step_accuracy\n",
    "        \n",
    "        epoch_loss += step_loss.item()\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc17ad6-0385-4ec3-b1f7-e1707142f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = load_optimizer(args, simclr_model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(log_dir='/home/kaipak/models/runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1801014-f200-494b-b4ec-627600db3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "args.global_step = 0\n",
    "args.current_epoch = 0\n",
    "args.logistic_epochs = 3\n",
    "tb_writer =  SummaryWriter(log_dir=f'/home/kaipak/models/tensorboard_logs/' +\n",
    "                           f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "#for epoch in range(args.logistic_epochs):\n",
    "for epoch in range(args.logistic_epochs):\n",
    "    loss_epoch, accuracy_epoch = train(args, train_loader, simclr_model, criterion, optimizer, tb_writer)\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{args.logistic_epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t Accuracy: {accuracy_epoch / len(train_loader)}\")\n",
    "    \n",
    "    args.current_epoch += 1\n",
    "\n",
    "loss_epoch, accuracy_epoch = test(\n",
    "    args, test_loader, simclr_model, criterion, optimizer\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[FINAL]\\t Loss: {loss_epoch / len(test_loader)}\\t Accuracy: {accuracy_epoch / len(test_loader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc80bb-b36c-4ac1-9857-0741e5738151",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1620d9-3049-424a-96e8-59fece309414",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model = SimCLRv2(resnet_depth=50, resnet_width_multiplier=2, sk_ratio=0.0625, \n",
    "                        pretrained_weights='/home/kaipak/models/SimCLRv2/r50_2x_sk1.pth')\n",
    "simclr_model.projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1f6c6-6acf-4e94-b875-44262b107ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77234e09-8f75-4fc8-9005-7012c5ae4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91205f9-175d-44e8-8878-4982c94ef242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
